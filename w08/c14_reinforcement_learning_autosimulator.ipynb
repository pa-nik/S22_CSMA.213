{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning - Auto Simulator\n",
    "\n",
    "Notebook to accompany *Grokking Artificial Intelligence Algorithms*, Chapter 10.  Adapted from Python source code  [auto_simulator.py](https://github.com/rishal-hurbans/Grokking-Artificial-Intelligence-Algorithms/blob/master/ch10-reinforcement_learning/auto_simulator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Reinforcement learning\n",
    "# The simulator should provide the following functionality and information as a minimum.\n",
    "\n",
    "# - Initialize the environment: This involves resetting the environment including the agent to the starting state.\n",
    "\n",
    "# - Get the current state of the environment: This function should provide the current state of the environment.\n",
    "# The state of the environment will change after each action is performed.\n",
    "\n",
    "# - Apply action to environment: This involves having the agent applying an action to the environment.\n",
    "# The environment will be impacted by the action which may result in a reward.\n",
    "\n",
    "# - Calculate the reward of an action: This is related to the \"apply action to environment” function.\n",
    "# The reward for the action and impact in the environment needs to be calculated.\n",
    "\n",
    "# - Determine if the goal is achieved: This function results in whether or not the agent has achieved the goal.\n",
    "# This can also sometimes be represented as “is complete”; in the case of an environment where the goal could\n",
    "# potentially not be achieved, the simulator needs to signal completion when it deems necessary.\n",
    "\n",
    "# Set constants that represent symbols, rewards, and actions in the environment\n",
    "ROAD_AGENT = '*'\n",
    "\n",
    "ROAD_EMPTY = ' '\n",
    "ROAD_EMPTY_REWARD = 100\n",
    "\n",
    "ROAD_OBSTACLE_CAR = '#'\n",
    "ROAD_OBSTACLE_CAR_REWARD = -100\n",
    "\n",
    "ROAD_OBSTACLE_PERSON = '!'\n",
    "ROAD_OBSTACLE_PERSON_REWARD = -1000\n",
    "\n",
    "ROAD_GOAL = '@'\n",
    "ROAD_GOAL_REWARD = 500\n",
    "\n",
    "ROAD_OUT_OF_BOUNDS = '-'\n",
    "ROAD_OUT_OF_BOUNDS_REWARD = -5\n",
    "\n",
    "COMMAND_NORTH = 0\n",
    "COMMAND_SOUTH = 1\n",
    "COMMAND_EAST = 2\n",
    "COMMAND_WEST = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random action\n",
    "def get_random_action():\n",
    "    return random.randint(0, 3)\n",
    "\n",
    "\n",
    "# Initialize a map with the starting point and goal\n",
    "DEFAULT_START_X = 0\n",
    "DEFAULT_START_Y = 0\n",
    "DEFAULT_GOAL_X = 9\n",
    "DEFAULT_GOAL_Y = 5\n",
    "DEFAULT_ROAD_SIZE_X = 10\n",
    "DEFAULT_ROAD_SIZE_Y = 10\n",
    "DEFAULT_ROAD = [[' ', '#', '#', '#', ' ', '#', ' ', ' ', '#', ' '],\n",
    "                [' ', '!', ' ', ' ', ' ', ' ', ' ', '!', ' ', '!'],\n",
    "                [' ', '#', '#', ' ', '#', '#', ' ', ' ', ' ', '#'],\n",
    "                [' ', ' ', ' ', ' ', ' ', ' ', '#', '#', ' ', '#'],\n",
    "                ['#', '!', '!', '#', ' ', '!', ' ', ' ', ' ', ' '],\n",
    "                [' ', ' ', ' ', '#', ' ', ' ', '#', '#', '#', ' '],\n",
    "                [' ', '#', '#', '#', '!', ' ', '#', '#', '!', ' '],\n",
    "                [' ', ' ', ' ', '!', '#', ' ', '!', ' ', ' ', ' '],\n",
    "                ['#', '!', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '#'],\n",
    "                [' ', '#', ' ', '#', '#', '@', ' ', '#', ' ', '!']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Simulator class encompasses the functions for the simulation environment\n",
    "class Simulator:\n",
    "\n",
    "    # Initialize the simulator with a road, its size, the starting point, and the goal point\n",
    "    def __init__(self, road, road_size_x, road_size_y, agent_start_x, agent_start_y, goal_x, goal_y):\n",
    "        self.road_size_x = road_size_x\n",
    "        self.road_size_y = road_size_y\n",
    "        self.road = road\n",
    "        self.rewards = 0\n",
    "        self.agent_x = agent_start_x\n",
    "        self.agent_y = agent_start_y\n",
    "        self.goal_x = goal_x\n",
    "        self.goal_y = goal_y\n",
    "        self.states = []\n",
    "\n",
    "    # Move the agent and return the reward based on a command. The command is an action (North, South, East, West)\n",
    "    def move_agent(self, command):\n",
    "        reward_update = 0\n",
    "        next_x = 0\n",
    "        next_y = 0\n",
    "        if command == COMMAND_NORTH:\n",
    "            next_x = self.agent_x - 1\n",
    "            next_y = self.agent_y\n",
    "        elif command == COMMAND_SOUTH:\n",
    "            next_x = self.agent_x + 1\n",
    "            next_y = self.agent_y\n",
    "        elif command == COMMAND_EAST:\n",
    "            next_x = self.agent_x\n",
    "            next_y = self.agent_y + 1\n",
    "        elif command == COMMAND_WEST:\n",
    "            next_x = self.agent_x\n",
    "            next_y = self.agent_y - 1\n",
    "        if self.is_within_bounds(next_x, next_y):\n",
    "            reward_update = self.calculate_movement_reward(next_x, next_y)\n",
    "            self.agent_x = next_x\n",
    "            self.agent_y = next_y\n",
    "            # print('Origin ', self.player_x, ',', self.player_y)\n",
    "            # print('Target ', next_x, ',', next_y)\n",
    "        else:\n",
    "            reward_update = ROAD_OUT_OF_BOUNDS_REWARD\n",
    "        self.rewards += reward_update\n",
    "        #print('Immediate Reward: ', reward_update)\n",
    "        #print('Lifetime Reward: ', self.rewards)\n",
    "        return reward_update\n",
    "\n",
    "    # Move the agent to a target point\n",
    "    def move(self, target_x, target_y):\n",
    "        self.agent_x = target_x\n",
    "        self.agent_y = target_y\n",
    "\n",
    "    # Calculate the reward for the movement based on an action\n",
    "    def calculate_movement_reward(self, next_x, next_y):\n",
    "        if self.road[next_x][next_y] == ROAD_OBSTACLE_PERSON:\n",
    "            return ROAD_OBSTACLE_PERSON_REWARD\n",
    "        elif self.road[next_x][next_y] == ROAD_OBSTACLE_CAR:\n",
    "            return ROAD_OBSTACLE_CAR_REWARD\n",
    "        elif self.road[next_x][next_y] == ROAD_GOAL:\n",
    "            return ROAD_GOAL_REWARD\n",
    "        else:\n",
    "            return ROAD_EMPTY_REWARD\n",
    "\n",
    "    # Determine if the target point is within bounds\n",
    "    def is_within_bounds(self, target_x, target_y):\n",
    "        if self.road_size_x > target_x >= 0 and self.road_size_y > target_y >= 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Determine if the goal is achieved\n",
    "    def is_goal_achieved(self):\n",
    "        if self.agent_x == self.goal_x and self.agent_y == self.goal_y:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Get the state. This is a string encoding based on the immediate neighbors around the current point\n",
    "    def get_state(self):\n",
    "        state = ''\n",
    "        for x in range(-1, 2):\n",
    "            for y in range(-1, 2):\n",
    "                if self.is_within_bounds(x, y):\n",
    "                    state += self.road[x][y]\n",
    "                else:\n",
    "                    state += ROAD_OUT_OF_BOUNDS\n",
    "        if state not in self.states:\n",
    "            self.states.append(state)\n",
    "        return self.states.index(state)\n",
    "\n",
    "    def print_road(self):\n",
    "        output = ''\n",
    "        for x in range(self.road_size_x):\n",
    "            for y in range(self.road_size_y):\n",
    "                if x == self.agent_x and y == self.agent_y:\n",
    "                    output += ROAD_AGENT\n",
    "                else:\n",
    "                    output += self.road[x][y]\n",
    "            output += '\\n'\n",
    "        print('Agent x: ', self.agent_x)\n",
    "        print('Agent y: ', self.agent_y)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a known most rewarding path\n",
    "def execute_happy_path():\n",
    "    simulator = Simulator(DEFAULT_ROAD, DEFAULT_ROAD_SIZE_X, DEFAULT_ROAD_SIZE_Y, DEFAULT_START_X, DEFAULT_START_Y,\n",
    "                          DEFAULT_GOAL_X, DEFAULT_GOAL_Y)\n",
    "    simulator.print_road()\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.print_road()\n",
    "\n",
    "\n",
    "# Execute a known path with penalties\n",
    "def execute_sad_path():\n",
    "    simulator = Simulator(DEFAULT_ROAD, DEFAULT_ROAD_SIZE_X, DEFAULT_ROAD_SIZE_Y, DEFAULT_START_X, DEFAULT_START_Y,\n",
    "                          DEFAULT_GOAL_X, DEFAULT_GOAL_Y)\n",
    "    simulator.print_road()\n",
    "    simulator.move_agent(COMMAND_NORTH)\n",
    "    simulator.move_agent(COMMAND_WEST)\n",
    "    simulator.move_agent(COMMAND_SOUTH)\n",
    "    simulator.move_agent(COMMAND_EAST)\n",
    "    simulator.print_road()\n",
    "\n",
    "\n",
    "# Execute a random brute force approach until the goal is found\n",
    "def execute_random_brute_force():\n",
    "    simulator = Simulator(DEFAULT_ROAD, DEFAULT_ROAD_SIZE_X, DEFAULT_ROAD_SIZE_Y, DEFAULT_START_X, DEFAULT_START_Y,\n",
    "                          DEFAULT_GOAL_X, DEFAULT_GOAL_Y)\n",
    "    simulator.print_road()\n",
    "    while not simulator.is_goal_achieved():\n",
    "        simulator.move_agent(get_random_action())\n",
    "        simulator.print_road()\n",
    "    simulator.print_road()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Q-table using Q-learning\n",
    "# - Initialize simulator:\n",
    "# This involves resetting the environment to the starting state with the agent in a neutral state.\n",
    "\n",
    "# - Get environment state:\n",
    "# This function should provide the current state of the environment. The state of the environment will change after\n",
    "# each action is performed.\n",
    "\n",
    "# - Is goal achieved:\n",
    "# Determine if the goal is achieved (or the simulator deems the exploration to be complete). In our example, this\n",
    "# would be picking up the owner of the self-driving car. If the goal is achieved, the algorithm will end.\n",
    "\n",
    "# - Pick a random action:\n",
    "# Determine if a random action should be selected; if so, a random action will be selected out of north, south, east,\n",
    "# and west. Random actions are useful to explore the possibilities in the environment instead of learn only a narrow\n",
    "# subset.\n",
    "\n",
    "# - Reference action in Q-table: If the decision to select a random action is not selected, the current environment\n",
    "# state is transposed to the Q-table and the respective action is selected based on the values in table. More about\n",
    "# the Q-table is coming up.\n",
    "\n",
    "# - Apply action to environment: This involves applying the selected action to the environment; whether that is a\n",
    "# random action or an action selected from the Q-table. An action will have a consequence in the environment and\n",
    "# yield a reward.\n",
    "\n",
    "# - Update Q-table: The following describes the concepts involved in updating the Q-table and the steps that are\n",
    "# carried out.\n",
    "# Learning rate is represented as alpha in the book.\n",
    "# Discount is represented as gamma in the book.\n",
    "def train_with_q_learning(observation_space, action_space, number_of_iterations, learning_rate, discount,\n",
    "                          chance_of_random_move):\n",
    "    # Initialize the Q-table\n",
    "    q_table = np.zeros([observation_space, action_space], dtype=np.int8)\n",
    "\n",
    "    # Repeat for a number of iterations\n",
    "    for i in range(number_of_iterations):\n",
    "        # Reset the simulator\n",
    "        simulator = Simulator(DEFAULT_ROAD, DEFAULT_ROAD_SIZE_X, DEFAULT_ROAD_SIZE_Y, DEFAULT_START_X, DEFAULT_START_Y,\n",
    "                              DEFAULT_GOAL_X, DEFAULT_GOAL_Y)\n",
    "        state = simulator.get_state()\n",
    "        done = False\n",
    "\n",
    "        # Continue while the simulator is not terminated\n",
    "        while not done:\n",
    "            action = COMMAND_SOUTH\n",
    "            # Choose a random action or choose the best move from the Q-table given the current state\n",
    "            if random.uniform(0, 1) > chance_of_random_move:\n",
    "                action = get_random_action()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            # Apply the selected action to the simulation and get the changed state and reward\n",
    "            reward = simulator.move_agent(action)\n",
    "            next_state = simulator.get_state()\n",
    "            done = simulator.is_goal_achieved()\n",
    "\n",
    "            print(simulator.get_state())\n",
    "            print('State: ', state)\n",
    "            print('Action: ', action)\n",
    "\n",
    "            # Calculate the Q-table value for the selected action\n",
    "            current_value = q_table[state, action]\n",
    "            next_state_max_value = np.max(q_table[next_state])\n",
    "            new_value = (1 - learning_rate) * current_value + learning_rate * (reward + discount * next_state_max_value)\n",
    "            q_table[state, action] = new_value\n",
    "            state = next_state\n",
    "            print(q_table)\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained Q-table to navigate the map to the goal, to test the training\n",
    "def execute_with_q_learning(q_table, number_of_episodes):\n",
    "    total_epochs = 0\n",
    "    total_penalties_person = 0\n",
    "    total_penalties_car = 0\n",
    "\n",
    "    # Repeat for a number of episodes\n",
    "    for episode in range(number_of_episodes):\n",
    "        # Initialize a simulator\n",
    "        simulator = Simulator(DEFAULT_ROAD,\n",
    "                              DEFAULT_ROAD_SIZE_X, DEFAULT_ROAD_SIZE_Y,\n",
    "                              DEFAULT_START_X, DEFAULT_START_Y,\n",
    "                              DEFAULT_GOAL_X, DEFAULT_GOAL_Y)\n",
    "        state = simulator.get_state()\n",
    "        epochs = 0\n",
    "        penalties_person = 0\n",
    "        penalties_car = 0\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Choose an action from the Q-table while the simulator is not terminated\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "            reward = simulator.move_agent(action)\n",
    "            state = simulator.get_state()\n",
    "            done = simulator.is_goal_achieved()\n",
    "\n",
    "            if reward == ROAD_OBSTACLE_PERSON_REWARD:\n",
    "                penalties_person += 1\n",
    "            elif reward == ROAD_OBSTACLE_CAR_REWARD:\n",
    "                penalties_car += 1\n",
    "\n",
    "            epochs += 1\n",
    "\n",
    "        # Calculate the penalties accrued\n",
    "        total_penalties_person += penalties_person\n",
    "        total_penalties_car += penalties_car\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print('Results after ', number_of_episodes, 'episodes:')\n",
    "    print('Average time steps per episode: ', total_epochs / number_of_episodes)\n",
    "    print('Average person penalties per episode: ', total_penalties_person / number_of_episodes)\n",
    "    print('Average car penalties per episode: ', total_penalties_person / number_of_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent x:  0\n",
      "Agent y:  0\n",
      "*### #  # \n",
      " !     ! !\n",
      " ## ##   #\n",
      "      ## #\n",
      "#!!# !    \n",
      "   #  ### \n",
      " ###! ##! \n",
      "   !# !   \n",
      "#!       #\n",
      " # ##@ # !\n",
      "\n",
      "Agent x:  9\n",
      "Agent y:  5\n",
      " ### #  # \n",
      " !     ! !\n",
      " ## ##   #\n",
      "      ## #\n",
      "#!!# !    \n",
      "   #  ### \n",
      " ###! ##! \n",
      "   !# !   \n",
      "#!       #\n",
      " # ##* # !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute_happy_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent x:  0\n",
      "Agent y:  0\n",
      "*### #  # \n",
      " !     ! !\n",
      " ## ##   #\n",
      "      ## #\n",
      "#!!# !    \n",
      "   #  ### \n",
      " ###! ##! \n",
      "   !# !   \n",
      "#!       #\n",
      " # ##@ # !\n",
      "\n",
      "Agent x:  1\n",
      "Agent y:  1\n",
      " ### #  # \n",
      " *     ! !\n",
      " ## ##   #\n",
      "      ## #\n",
      "#!!# !    \n",
      "   #  ### \n",
      " ###! ##! \n",
      "   !# !   \n",
      "#!       #\n",
      " # ##@ # !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "execute_sad_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_random_brute_force()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.6\n",
    "CHANCE_OF_RANDOM_ACTION = 0.1\n",
    "trained_q_table = train_with_q_learning(4*4*4*4*4*4*4*4, 4, 100, LEARNING_RATE, DISCOUNT, CHANCE_OF_RANDOM_ACTION)\n",
    "execute_with_q_learning(trained_q_table, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d4e034db59a6bda3f881a616c0934516fd717a906758f1f7139e3bd1294e4c5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
